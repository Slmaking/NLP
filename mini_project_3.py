# -*- coding: utf-8 -*-
"""Mini project 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nB-GL4ugwsw9gyOUPFvjfUsRBH8Xmu_g

## **Mini Project 3: Done by Rahma Nouaji, Mohammad Ghavidel, Bita Farokhian**

# Data collection and preprocessing
"""

!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

import pandas as pd
import numpy as np
import re
from sklearn.preprocessing import LabelEncoder
import math
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
# download stopwords and lemmatizer from nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

import os
import glob

train_dir = 'aclImdb/train'
pos_files = glob.glob(os.path.join(train_dir, 'pos', '*.txt'))
neg_files = glob.glob(os.path.join(train_dir, 'neg', '*.txt'))
x_train=[]
y_train=[]
# Load positive reviews
for filename in pos_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_train.append(review)
        y_train.append('positive')
# Load negative reviews
for filename in neg_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_train.append(review)
        y_train.append('Negative')


# Create Pandas DataFrame for training data
train_df = pd.DataFrame({'text': x_train, 'sentiment': y_train})

import os
import glob

test_dir = 'aclImdb/test'
pos_files = glob.glob(os.path.join(test_dir, 'pos', '*.txt'))
neg_files = glob.glob(os.path.join(test_dir, 'neg', '*.txt'))
x_test=[]
y_test=[]
# Load positive reviews
for filename in pos_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_test.append(review)
        y_test.append('positive')
# Load negative reviews
for filename in neg_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_test.append(review)
        y_test.append('Negative')


# Create Pandas DataFrame for training data
test_df = pd.DataFrame({'text': x_test, 'sentiment': y_test})

df = pd.concat([train_df, test_df], ignore_index=True)

df.describe()

# Preprocessing the data for naive bayes
def remove_tags(string):
    result = re.sub(r'<[^>]+>', '', string)  # remove HTML tags
    result = re.sub(r'https?://\S+', '', result)  # remove URLs
    return result

def remove_special_characters(text, remove_digits=True):
    pattern = r'[^a-zA-Z0-9_\s]'  # include underscore and remove non-alphanumeric characters
    if remove_digits:
        pattern = r'[^a-zA-Z_\s]'  # include underscore and remove non-alphanumeric characters and digits
    text = re.sub(pattern, '', text)
    return text


def tokenize(text):
    tokens = word_tokenize(text)
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    return stemmed_tokens

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if not word in stop_words]
    return filtered_tokens

def lemmatize(tokens):
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return lemmatized_tokens
# preprocess the text data
df['text'] = df['text'].apply(remove_tags)
df['text'] = df['text'].apply(remove_special_characters)
df['text'] = df['text'].apply(lambda x: x.lower())
df['tokens'] = df['text'].apply(tokenize)
df['tokens'] = df['tokens'].apply(remove_stopwords)
df['tokens'] = df['tokens'].apply(lemmatize)


# create a bag of words representation using CountVectorizer
#vectorizer = CountVectorizer()
#bow = vectorizer.fit_transform(train_df['tokens'].apply(lambda x: ' '.join(x)))

df.head()

labels = df['sentiment'].values
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)

df['sentiment']=encoded_labels

df.head()

X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['sentiment'], test_size=0.2, random_state=42)

# Convert preprocessed training data to bag of words representation
vectorizer = CountVectorizer()
X_train_bow = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))
vectorizer.get_feature_names_out()

# Transform preprocessed testing data to bag of words representation using the same CountVectorizer object
X_test_bow = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))

X_train.shape

print(X_train_bow.shape)

"""# Naive Bayes model

## Gaussian Naive Bayes
"""

X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['sentiment'], test_size=0.2, random_state=42)

# Convert preprocessed training data to bag of words representation
vectorizer = CountVectorizer(binary=True, max_features=10000)
X_train_bow = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))
vectorizer.get_feature_names_out()

# Transform preprocessed testing data to bag of words representation using the same CountVectorizer object
X_test_bow = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))


# Train a Gaussian Naive Bayes classifier
clf = GaussianNB()
clf.fit(X_train_bow.toarray(), y_train)

# Test the classifier on the testing set
y_pred = clf.predict(X_test_bow.toarray())
# evaluate the performance of the classifier
accuracy = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred)
recall = metrics.recall_score(y_test, y_pred)
f1_score = metrics.f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)

"""## Multinomial Naive Bayes"""

class MultinomialNaiveBayes:
    def __init__(self, alpha=1,max_features=None): 
        self.max_features = max_features
        self.vectorizer = CountVectorizer(max_features=max_features)
        self.alpha = alpha  # Laplace smoothing hyperparameter
       

    
        
    def fit(self, X, y):
        
        self.classes = np.unique(y)
      
        X_train_bow = self.vectorizer.fit_transform(X.apply(lambda x: ' '.join(x)))
        self.num_classes = len(self.classes)
        self.vocab = self.vectorizer.get_feature_names_out() # get vocabulary from CountVectorizer
        self.log_priors= np.zeros(self.num_classes)
        self.log_likelihoods= np.zeros((self.num_classes, len(self.vocab)))
        # print("X_train_bow shape:", X_train_bow.shape)
        # print("Number of training examples:", X_train_bow.shape[0])
        for i, c in enumerate(self.classes):
            X_c = X_train_bow[y == c]
            # print("X_c shape for class", c, ":", X_c.shape)
            self.log_priors[i] = np.log(X_c.shape[0] / X_train_bow.shape[0])
            a = np.sum(X_c, axis=0) + self.alpha
            b = np.sum(X_c) + self.alpha
            
            self.log_likelihoods[i, :] = np.log(a / b)


        
    def predict(self, X_test):
        X_test = self.vectorizer.transform(X_test.apply(lambda x: ' '.join(x))) # transform X_test using CountVectorizer
        log_posteriors = np.zeros((X_test.shape[0], self.num_classes))
        
        for i in range(self.num_classes):
            
            log_posteriors[:, i] = np.sum((self.log_likelihoods[i, :][:, np.newaxis].T * X_test.T).T, axis=1) + self.log_priors[i]
            # print(self.log_likelihoods_[i, :].shape)
            # print(X_test.shape)
        return self.classes[np.argmax(log_posteriors, axis=1)]

    def score(self, X_test, y_test):
        y_pred = self.predict(X_test)
        correct_predictions = np.sum(y_pred == y_test)
        total_predictions = len(y_test)
        return correct_predictions / total_predictions

    def get_params(self, deep=True):
        return {"alpha": self.alpha}
    
    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self
    def evaluate_acc(self, X_test, y_test):
        y_pred = self.predict(X_test)
        correct_predictions = np.sum(y_pred == y_test)
        total_predictions = len(y_test)
        return correct_predictions / total_predictions

"""### Grid Search ( Laplace smoothing , max_features)"""

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold 

# Define the parameter grid
param_grid = {
    'alpha': [0.1, 0.5, 1.0,10],
    'max_features': [5000, 10000, 50000,None]
}

# Create a MultinomialNaiveBayes object
nb = MultinomialNaiveBayes()


skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)
n_splits=5
# Fit the GridSearchCV object to the data
results = []
# Loop over the hyperparameters and train the model
for alpha in param_grid['alpha']:
    for max_features in param_grid['max_features']:
       nb=MultinomialNaiveBayes(alpha=alpha, max_features=max_features)
       # Initialize variables to keep track of the average score over all folds
       total_score = []
       n_splits = 0
       for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):
            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]
            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]
        
  
            nb.fit(X_train_fold, y_train_fold)
            score = nb.evaluate_acc(X_val_fold, y_val_fold)
            total_score.append(score)
       # Compute the average score over all folds
       avg_score = np.mean(total_score)
       std_score = np.std(total_score)

       # Print the current hyperparameters, score, and standard deviation
       print("alpha:", alpha, "max_features:", max_features, "score:", avg_score, "std:", std_score)
        
        # Append the current hyperparameters, score, and standard deviation to the results list
       results.append({"alpha": alpha, "max_features": max_features, "score": avg_score, "std": std_score})
# Convert results to a pandas dataframe and print
results_df = pd.DataFrame(results)
print(results_df)

# Find the row with the highest score
best_row = results_df.loc[results_df['score'].idxmax()]

# Extract the hyperparameters and score from the best row
best_alpha = best_row['alpha']
best_max_features = best_row['max_features']
best_score = best_row['score']
best_std = best_row['std']

# Print the hyperparameters and score of the best row
print("Best alpha:", best_alpha)
print("Best max_features:", best_max_features)
print("Best score:", best_score)
print("Standard Deviation of Best Score:", best_std)

"""### Learning curve"""

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
nb =  MultinomialNaiveBayes(alpha=1, max_features=None)
train_sizes, train_scores, test_scores = learning_curve(
    estimator=nb,
    X=X_train,
    y=y_train,
    train_sizes=[0.1, 0.3, 0.5, 0.7, 0.9],
    cv=5
)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.1,
                 color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.1,
                 color="g")

plt.xlabel("Training examples")
plt.ylabel("Score")
plt.legend(loc="best")
plt.show()

"""### Cross validation curve for alpha hyperparameter"""

from sklearn.model_selection import GridSearchCV
alpha_ranges = {
    'alpha': [0.1, 0.5, 1.0,10]
}



# Create an instance of the MultinomialNaiveBayes model
model = MultinomialNaiveBayes(max_features=None)

# Create a GridSearchCV object with the model and parameter grid
grid_search = GridSearchCV(model, param_grid=alpha_ranges, scoring='accuracy', cv=5, return_train_score=True)

# Fit the grid search object to the training data
grid_search.fit(X_train, y_train)

# Print the best parameter value and accuracy score
print("Best alpha:", grid_search.best_params_['alpha'])
print("Best accuracy:", grid_search.best_score_)

train_acc = grid_search.cv_results_['mean_train_score']
train_std = grid_search.cv_results_['std_train_score']

test_acc = grid_search.cv_results_['mean_test_score']
test_std = grid_search.cv_results_['std_test_score']

import matplotlib.pyplot as plt
alpha_ranges = {
    'alpha': [0.1, 0.5, 1.0,10]
}

plt.plot(alpha_ranges['alpha'], train_acc, "o-", color="b", label="Training Score")
plt.plot(alpha_ranges['alpha'], test_acc, "o-", color="g", label="Cross Validation Score")
plt.fill_between(alpha_ranges['alpha'], train_acc-train_std, train_acc+train_std, alpha=0.2, color="b")
plt.fill_between(alpha_ranges['alpha'], test_acc-test_std, test_acc+test_std, alpha=0.2, color="g")
plt.title("Validation Curve with Naive Bayes Classifier")
plt.xlabel("Alpha")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.show()

"""### Model evaluation ( Accuracy, Precision, Recall, F1Score, Confusion matrix)"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

nb = MultinomialNaiveBayes(alpha=1, max_features=None)

# fit NaiveBayes model on training data
nb.fit(X_train, y_train)

# make predictions on test data
y_pred = nb.predict(X_test)
# evaluate model performance using accuracy score
accuracy = nb.evaluate_acc(X_test, y_test)
accuracy_sklearn = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred)
recall = metrics.recall_score(y_test, y_pred)
f1_score = metrics.f1_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')
print(f'Precision: {precision:.3f}')
print(f'Recall: {recall:.3f}')
print(f'F1Score: {f1_score:.3f}')

print(f'Accuracy from sklearn: {accuracy_sklearn:.3f}')
# Create the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)

import seaborn as sns
import matplotlib.pyplot as plt
# generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# plot the heatmap
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

def compute_metrics(confusion_matrix):
    # Compute precision, recall, and F1-score
    tp = confusion_matrix[0][0]
    fp = confusion_matrix[0][1]
    fn = confusion_matrix[1][0]
    tn = confusion_matrix[1][1]
    
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * precision * recall / (precision + recall)
    
    return precision, recall, f1

precision, recall, f1 = compute_metrics(cm)
print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1-score: {:.2f}".format(f1))

"""#Bert model"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

#downloading pretrained BERT
!pip install pytorch_pretrained_bert
!pip install pytorch-nlp
!pip install transformers

import sys
import numpy as np
import random as rn
import pandas as pd
import torch
from pytorch_pretrained_bert import BertModel
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import *
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
# from torchnlp.datasets import imdb_dataset   
from pytorch_pretrained_bert import BertTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.optim import Adam
from torch.nn.utils import clip_grad_norm_
from IPython.display import clear_output
import matplotlib.pyplot as plt

#import the train data
import os
import glob

train_dir = 'aclImdb/train'
pos_files = glob.glob(os.path.join(train_dir, 'pos', '*.txt'))
neg_files = glob.glob(os.path.join(train_dir, 'neg', '*.txt'))
x_train=[]
y_train=[]
# Load positive reviews
for filename in pos_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_train.append(review)
        y_train.append('positive')
# Load negative reviews
for filename in neg_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_train.append(review)
        y_train.append('Negative')


# Create Pandas DataFrame for training data
train_ds = pd.DataFrame({'text': x_train, 'sentiment': y_train})

#import the test data
import os
import glob

test_dir = 'aclImdb/test'
pos_files = glob.glob(os.path.join(test_dir, 'pos', '*.txt'))
neg_files = glob.glob(os.path.join(test_dir, 'neg', '*.txt'))
x_test=[]
y_test=[]
# Load positive reviews
for filename in pos_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_test.append(review)
        y_test.append('positive')
# Load negative reviews
for filename in neg_files:
    with open(filename, 'r', encoding='utf-8') as f:
        review = f.read()
        x_test.append(review)
        y_test.append('Negative')


# Create Pandas DataFrame for training data
test_ds = pd.DataFrame({'text': x_test, 'sentiment': y_test})

#merging the two dataFrame
ds = pd.concat([train_ds, test_ds], ignore_index=True)
ds = ds.sample(frac=1)

train_ds['sentiment'].value_counts()

#make data categorical
from sklearn.preprocessing import LabelEncoder

labels = ds['sentiment'].values
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)
ds['sentiment']=encoded_labels

"""##Dataset Cleaning and Preprocessing for BERT

"""

#Libraries
import re
import nltk
from nltk import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('punkt')
stops = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

#Preprocessing Function
def mystopwords(text):
    return ' '.join([w for w in word_tokenize(text) if not w in stops])

def clean_text(string):
  clean=re.sub('[\n]',' ',string)
  clean=re.sub('[^a-zA-Z]',' ',clean)
  clean=mystopwords(clean)
  return clean

def remove_tags(string):
    result = re.sub(r'<[^>]+>', '', string)  # remove HTML tags
    result = re.sub(r'https?://\S+', '', result)  # remove URLs
    return result

def remove_special_characters(text, remove_digits=True):
    pattern = r'[^a-zA-Z0-9_\s]'  # include underscore and remove non-alphanumeric characters
    if remove_digits:
        pattern = r'[^a-zA-Z_\s]'  # include underscore and remove non-alphanumeric characters and digits
    text = re.sub(pattern, '', text)
    return text


def tokenize(text):
    tokens = word_tokenize(text)
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    return stemmed_tokens

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if not word in stop_words]
    return filtered_tokens

def lemmatize(tokens):
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return lemmatized_tokens

ds['text'] = ds['text'].apply(remove_tags)
ds['text'] = ds['text'].apply(remove_special_characters)
ds['text'] = ds['text'].apply(lambda x: x.lower())
#train_tokens = train_sentences.apply(tokenize)
#train_tokens = train_sentences.apply(remove_stopwords)
#train_tokens = train_sentences.apply(lemmatize)
#test_tokens = test_sentences.apply(tokenize)
#test_tokens = test_sentences.apply(remove_stopwords)
#test_tokens = test_sentences.apply(lemmatize)

ds[0:]

ds['text'][0]

ds.head()

# Libraries 
import matplotlib.pyplot as plt
# Bar Chart for Target Disribution
plt.title('Target Frequency')
label= ('pos', 'neg')
frequnecy= train_ds['sentiment'].value_counts()
plt.bar(frequnecy.index, frequnecy.values, tick_label=label, color = ['lightblue','darkorange'])
plt.grid(True)
plt.show()

"""we will use 10% of the dataset for implementing the model"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

#Libraries 
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
#function 
def display_wordcloud(data, title):
    words_list = data.unique().tolist()
    words = ' '.join(words_list)
    
    wordcloud = WordCloud(width = 1000, height = 600,stopwords = set(STOPWORDS), background_color='white').generate(words)
    plt.figure(figsize=(40, 24))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()

display_wordcloud(ds[ds['sentiment']==1]['text'], 'positive')

display_wordcloud(ds[ds['sentiment']==0]['text'],['Negative'])

"""##**BERT Model**"""

device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )
print(device)

rn.seed(7)
np.random.seed(7)
torch.manual_seed(7)
torch.cuda.manual_seed(7)

# Import required libraries
import random
import numpy as np
import torch
import transformers
from transformers import BertForSequenceClassification, BertTokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# Load the IMDB comment data

from sklearn.datasets import load_files
imdb_data =ds

imdb_data

# Split the data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(imdb_data['text'].values.tolist(), np.array(imdb_data['sentiment']), random_state=7, shuffle= True, test_size=0.2)

train_texts[0]

train_labels[0]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
MAX_LEN = 128      #for not consuming much resources
RANDOM_SEED = 7
device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )
print(device)

# Tokenize input text using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']).to(device),
                              torch.tensor(train_encodings['attention_mask']).to(device),
                              torch.tensor(train_labels).to(device))
val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']).to(device),
                            torch.tensor(val_encodings['attention_mask']).to(device),
                            torch.tensor(val_labels).to(device))

# Set up training parameters
batch_size = 32 
Epochs = 4
learning_rate = 2e-5

# Set up data loaders for batching
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Initialize the Bert model and optimizer
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)

# Define BERT Model
def BERT_MODEL(model, optimizer, train_loader,val_loader,Epoches):
  avg_accuracy_tr = []
  avg_accuracy_t = []
  for epochs in range(Epoches):
    print('Epoch number : {}'.format(epochs+1))
    total_loss_tr, total_accuracy_tr = 0, 0
    total_preds_tr = []
    total_loss_t, total_accuracy_t = 0, 0
    total_preds_t = []
    #train Model
    model.train()
    for step, batch in enumerate(train_loader):
      batch = tuple(t.to(device) for t in batch)
      inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}
      optimizer.zero_grad()

      outputs = model(**inputs)

      loss = outputs[0]
      logits = outputs[1]
      total_loss_tr += loss.item()
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
      optimizer.step()
      preds = torch.argmax(logits, dim=1).flatten()
      total_preds_tr += preds.tolist()
      labels = inputs['labels'].flatten()
      total_accuracy_tr += accuracy_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy())

    avg_loss = total_loss_tr / len(train_loader)
    avg_accuracy_tr.append(total_accuracy_tr / len(train_loader))
        # Evaluate Model
    model.eval()
    for step, batch in enumerate(val_loader):
      batch = tuple(t.to(device) for t in batch)
      inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}
      with torch.no_grad():
        outputs = model(**inputs)

      loss = outputs[0]
      logits = outputs[1]
      total_loss_t += loss.item()

      preds = torch.argmax(logits, dim=1).flatten()
      total_preds_t += preds.tolist()
      labels = inputs['labels'].flatten()
      total_accuracy_t += accuracy_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy())
    avg_accuracy_t.append(total_accuracy_t/ len(val_loader))

  return  avg_accuracy_tr,avg_accuracy_t

accu_train,accu_test = BERT_MODEL(model, optimizer, train_loader,val_loader,Epochs)

import seaborn as sns
import matplotlib.pyplot as plt

# Create a Pandas DataFrame with the data
data = pd.DataFrame({
    'epoch': [1, 2, 3, 4],
    'train_acc': accu_train,
    'test_acc': accu_test
})

# Melt the DataFrame to create a long-form version
data_melt = pd.melt(data, id_vars=['epoch'], var_name='dataset', value_name='accuracy')

# Plot the data using Seaborn
sns.lineplot(x='epoch', y='accuracy', hue='dataset', style='dataset', markers=True, data=data_melt)

# Add labels and legend
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy per Epoch in BERT Model')
plt.legend(title='Dataset')

# Display the plot
plt.show()

"""### Grid Search"""

Max_length_range = [128,256,512]
Batch_Sizes =  [4, 8, 16, 32]
Learning_rate_range = [1e-5,2e-5,5e-5]
epochs = 4

for Max_l in Max_length_range:
  train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=Max_l)
  val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=Max_l)
  train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']).to(device),
                                torch.tensor(train_encodings['attention_mask']).to(device),
                                torch.tensor(train_labels).to(device))
  val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']).to(device),
                              torch.tensor(val_encodings['attention_mask']).to(device),
                              torch.tensor(val_labels).to(device))
  for Batches in Batch_Sizes:
    epochs = 4
# Set up data loaders for batching
    train_loader = DataLoader(train_dataset, batch_size=Batches, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Batches)
# Initialize the Bert model and optimizer
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)
    for learning_rate in Learning_rate_range: 
      optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)
# Define training function
      def train(model, optimizer, train_loader):
        model.train()
        total_loss, total_accuracy = 0, 0
        total_preds = []
        for step, batch in enumerate(train_loader):
          #print(step)
          batch = tuple(t.to(device) for t in batch)
          inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}
          optimizer.zero_grad()

          outputs = model(**inputs)

          loss = outputs[0]
          logits = outputs[1]
          total_loss += loss.item()
          loss.backward()

          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

          optimizer.step()

          preds = torch.argmax(logits, dim=1).flatten()
          total_preds += preds.tolist()
          labels = inputs['labels'].flatten()
          total_accuracy += accuracy_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy())

        avg_loss = total_loss / len(train_loader)
        avg_accuracy = total_accuracy / len(train_loader)
        return avg_loss, avg_accuracy, total_preds

# Define evaluation function
      def evaluate(model, val_loader):
        model.eval()
        total_loss, total_accuracy = 0, 0
        total_preds = []

        for step, batch in enumerate(val_loader):
          batch = tuple(t.to(device) for t in batch)
          inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}
          with torch.no_grad():
            outputs = model(**inputs)

          loss = outputs[0]
          logits = outputs[1]
          total_loss += loss.item()

          preds = torch.argmax(logits, dim=1).flatten()
          total_preds += preds.tolist()
          labels = inputs['labels'].flatten()
          total_accuracy += accuracy_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy())
        return total_accuracy/ len(val_loader)
      avg_loss, avg_accuracy, total_preds = train(model, optimizer, train_loader)
      avg_accuracy_test = evaluate(model, val_loader)
      print(" Test Accuracy For Max Length =  {} , Batch Size = {} , Learning Rate = {}   is  {} ".format(Max_l, Batches, learning_rate,avg_accuracy_test))



"""### Attention matrix"""

#libraries
from keras.layers import Layer
import keras.backend as K
import keras
import matplotlib.pyplot as plt
import numpy as np

!pip install transformers

ds['text'][0]

x= tokenizer.tokenize("one more of those brilliant young men who went all out dared to make a teen romance film can i actually call")

def visualize_token2token_scores(scores_mat, x_label_name='Head'):
    fig = plt.figure(figsize=(20, 20))

    for idx, scores in enumerate(scores_mat):
        scores_np = np.array(scores)
        ax = fig.add_subplot(4, 3, idx+1)
        # append the attention weights
        im = ax.imshow(scores, cmap='viridis')

        fontdict = {'fontsize': 10}

        ax.set_xticks(range(len(x)))
        ax.set_yticks(range(len(x)))

        ax.set_xticklabels(x, fontdict=fontdict, rotation=90)
        ax.set_yticklabels(x, fontdict=fontdict)
        ax.set_xlabel('{} {}'.format(x_label_name, idx+1))

        fig.colorbar(im, fraction=0.046, pad=0.04)
    plt.tight_layout()
    plt.show()

from transformers import AutoTokenizer, AutoModel, utils
utils.logging.set_verbosity_error()  # Suppress standard warnings
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased", output_attentions=True)

!pip install bertviz

"""**Correctly predicted sentences**"""

ds['text'][0]

"""first sentence for implementing attention: "one more of those brilliant young men who went all out and dared to make a teen romance film can i actually call it that it would invoke the devil out of its fanson a micro budget but packed with such taste sensitiveness and maturity peter sollett you deserve more admiration and respectthanks once again for demonstrating to the powers that be in the industry that stereotypes can be flushed down the toilet one locationa handful of rich characters low budgetgood actingand that too amateursdecent lighting  worshippers of true indie cinema should watch more of this and stop watchingwellyou know what"
"""

# Import specialized versions of models (that return query/key vectors)
from bertviz.transformers_neuron_view import BertModel, BertTokenizer
from bertviz.neuron_view import show

model_type = 'bert'
model_version = 'bert-base-uncased'
do_lower_case = True
sentence_a = "one more of those brilliant young men who went all out"
sentence_b = "dared to make a teen romance film can i actually call"
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='light', layer=2, head=0)

import torch
from transformers import BertModel, BertTokenizer

model_version = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_version)
model = BertModel.from_pretrained(model_version, output_attentions=True)

# Set the layer and head to extract attention weights from
layer = 2
head = 0

# Tokenize the input sentences
inputs = tokenizer.encode_plus("one more of those brilliant young men who went all out", "dared to make a teen romance film can i actually call", add_special_tokens=False, return_tensors='pt')

# Obtain the attention weights from the model
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions[layer][head]

# Print the attention weights
print(attentions)

visualize_token2token_scores(attentions.squeeze().detach().cpu().numpy())

# Load model and retrieve attention weights

from bertviz import head_view, model_view
from transformers import BertTokenizer, BertModel

model_version = 'bert-base-uncased'
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version)
sentence_a = "one more of those brilliant young men who went all out"
sentence_b = "dared to make a teen romance film can i actually call"
inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')
input_ids = inputs['input_ids']
token_type_ids = inputs['token_type_ids']
attention = model(input_ids, token_type_ids=token_type_ids)[-1]
sentence_b_start = token_type_ids[0].tolist().index(1)
input_id_list = input_ids[0].tolist() # Batch index 0
tokens = tokenizer.convert_ids_to_tokens(input_id_list)

attention

model_view(attention, tokens, sentence_b_start)

"""**Incorrectly Predicted Sentence**"""

ds['text'][1]

ds['sentiment'][1]

import torch
from transformers import BertModel, BertTokenizer

model_version = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_version)
model = BertModel.from_pretrained(model_version, output_attentions=True)

# Set the layer and head to extract attention weights from
layer = 2
head = 0

# Tokenize the input sentences
inputs = tokenizer.encode_plus("one more of those brilliant young men who went all out", "dared to make a teen romance film can i actually call", add_special_tokens=False, return_tensors='pt')

# Obtain the attention weights from the model
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions[layer][head]

# Print the attention weights
print(attentions)

y= tokenizer.tokenize("I regard it as a favoured film of all time and I feel and believe in the characters of this movie")

def visualize_token2token_scores(scores_mat, x_label_name='Head'):
    fig = plt.figure(figsize=(20, 20))

    for idx, scores in enumerate(scores_mat):
        scores_np = np.array(scores)
        ax = fig.add_subplot(4, 3, idx+1)
        # append the attention weights
        im = ax.imshow(scores, cmap='viridis')

        fontdict = {'fontsize': 10}

        ax.set_xticks(range(len(y)))
        ax.set_yticks(range(len(y)))

        ax.set_xticklabels(y, fontdict=fontdict, rotation=90)
        ax.set_yticklabels(y, fontdict=fontdict)
        ax.set_xlabel('{} {}'.format(x_label_name, idx+1))

        fig.colorbar(im, fraction=0.046, pad=0.04)
    plt.tight_layout()
    plt.show()

visualize_token2token_scores(attentions.squeeze().detach().cpu().numpy())

# Import specialized versions of models (that return query/key vectors)
from bertviz.transformers_neuron_view import BertModel, BertTokenizer
from bertviz.neuron_view import show

model_type = 'bert'
model_version = 'bert-base-uncased'
do_lower_case = True
sentence_a = "I regard it as a favoured film of all time"
sentence_b = "I feel and believe in the characters of this movie"
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='light', layer=2, head=0)